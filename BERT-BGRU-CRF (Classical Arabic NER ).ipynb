{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CA_BERT_BGRU_CRF_run5.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.7"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "dP-0hZWKhOFQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "463592d6-e132-4006-d882-19b4406fb27c"
      },
      "source": [
        "pip install transformers==3"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers==3 in /usr/local/lib/python3.6/dist-packages (3.0.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers==3) (2019.12.20)\n",
            "Requirement already satisfied: tokenizers==0.8.0-rc4 in /usr/local/lib/python3.6/dist-packages (from transformers==3) (0.8.0rc4)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (from transformers==3) (0.1.95)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers==3) (2.23.0)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers==3) (0.0.43)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers==3) (20.9)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers==3) (3.0.12)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers==3) (4.41.1)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers==3) (0.8)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers==3) (1.19.5)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==3) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==3) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==3) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==3) (2.10)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==3) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==3) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==3) (1.0.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers==3) (2.4.7)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oZ30Sbz5RPXU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "163b9912-ca5b-4fd0-85f8-34155fc16eb4"
      },
      "source": [
        "pip install seqeval"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: seqeval in /usr/local/lib/python3.6/dist-packages (1.2.2)\n",
            "Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.6/dist-packages (from seqeval) (0.22.2.post1)\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.6/dist-packages (from seqeval) (1.19.5)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.0.0)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.4.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MQOgfUudAmOG",
        "outputId": "9c8a9921-3945-4509-b927-321a6ac902a7"
      },
      "source": [
        "pip install pytorch-text-crf"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pytorch-text-crf in /usr/local/lib/python3.6/dist-packages (0.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from pytorch-text-crf) (1.7.0+cu101)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch->pytorch-text-crf) (3.7.4.3)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch->pytorch-text-crf) (0.16.0)\n",
            "Requirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from torch->pytorch-text-crf) (0.8)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch->pytorch-text-crf) (1.19.5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ljlZy6SV4AkP"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "import re\n",
        "import time\n",
        "import random\n",
        "import functools\n",
        "from crf.crf import ConditionalRandomField"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o52iq5dUhCzs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8063df36-faf3-4cf2-ecd2-8175ab522033"
      },
      "source": [
        "# If there's a GPU available...\n",
        "if torch.cuda.is_available():    \n",
        "\n",
        "    # Tell PyTorch to use the GPU.    \n",
        "    device = torch.device(\"cuda\")\n",
        "\n",
        "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
        "\n",
        "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
        "    !nvidia-smi\n",
        "\n",
        "# If not...\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There are 1 GPU(s) available.\n",
            "We will use the GPU: Tesla P100-PCIE-16GB\n",
            "Tue Feb 16 15:56:49 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.39       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   44C    P0    31W / 250W |      2MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D5dwnfw7g-rm"
      },
      "source": [
        "import re\n",
        "dump_chars = '!\"#$%&\\'()*+,-./:;<=>?@[\\]^_`{|}~’،ـ؟؛«» '\n",
        "\n",
        "def clean_word(word):\n",
        "    word = word.translate(str.maketrans({key: None for key in dump_chars}))\n",
        "    \n",
        "    #remove tashkeel\n",
        "    p_tashkeel = re.compile(r'[\\u0617-\\u061A\\u064B-\\u0652]')\n",
        "    word = re.sub(p_tashkeel,\"\", word)\n",
        "    \n",
        "    return word\n",
        "\n",
        "a = ['أ','إ','آ']\n",
        "\n",
        "def load_data(filename):\n",
        "    f = open(filename, 'r',encoding ='utf-8')\n",
        "    sents = f.read().split('\\n. O\\n')\n",
        "    f.close()\n",
        "    # tokenize words\n",
        "    words = [None]*len(sents)\n",
        "    tokens = [None]*len(sents)\n",
        "    for i, sent in enumerate(sents):\n",
        "        sent = sent.split('\\n')\n",
        "        words[i] = []\n",
        "        tokens[i] = []\n",
        "        for word in sent:\n",
        "            line = word.rsplit(' ', 1)\n",
        "            line[0] = clean_word(line[0])\n",
        "            if line[1] != 'O':\n",
        "              line[1] = \"B-\" + line[1]\n",
        "            if line [1] not in tag_classes:\n",
        "              line[1] = 'O'\n",
        "            if len(line[0]) > 0:\n",
        "                words[i].append(line[0])\n",
        "                tokens[i].append(line[1])\n",
        "                    \n",
        "                \n",
        "    return [d for d in words if len(d) > 0], [d for d in tokens if len(d) > 0]\n",
        "\n",
        "# load data\n",
        "tag_classes = ['B-Clan', 'O', 'B-Prophet',  'B-Pers', 'B-Date',  'B-Allah', 'B-NatOb', 'B-Loc']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hxY2VPnROAjg"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "sents, labels = load_data ('CANERCorpus-38maxlen.txt')\n",
        "train_sents, test_sents, train_labels, test_labels = train_test_split(sents, labels, test_size=0.1, random_state=2018)\n",
        "train_sents, val_sents, train_labels, val_labels = train_test_split(train_sents, train_labels, test_size=0.1, random_state=2018)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tmdTcVopBwYK"
      },
      "source": [
        "tag = []\n",
        "\n",
        "for i , label in enumerate (labels) :\n",
        "  for j , l in enumerate (label):\n",
        "    if (l == 'B-Crime'):\n",
        "      tag.append(sents[i][j])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9PmQBmQECOiQ",
        "outputId": "e4bdc7e8-1cfa-420f-a056-589c80c56ec0"
      },
      "source": [
        "\n",
        "tag = set (tag)\n",
        "print (len (tag))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "huUcTbCtgChe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c3f5f997-4298-4ce3-8531-6a23b352460f"
      },
      "source": [
        "print (test_sents[0])\n",
        "print (test_labels[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['تعتد', 'عند', 'أهل', 'الزوج', 'أربعة', 'أشهر', 'وعشرة', 'أيام', 'وصية', 'يوصون', 'أن', 'تبقى', 'في', 'دار', 'أهل', 'الزوج', 'إلى', 'تمام', 'السنة', 'متاعا', 'يمتعن', 'متاعا', 'بالسكنى', 'والنفقة', 'في', 'تركته', 'الحول', 'سنة', 'كاملة', 'غير', 'إخراج', 'لا', 'يخرجن', 'جناح', 'إثم', 'جاء', 'الميراث', 'أي']\n",
            "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "weK7SWOe1u4o",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3bf797da-e038-4a9c-97c7-fee8383c6209"
      },
      "source": [
        "print('Max train sentence length:',len(max(train_sents, key=len)))\n",
        "print('Max val sentence length:',len(max(val_sents, key=len)))\n",
        "print('Max test sentence length:',len(max(test_sents, key=len)))\n",
        "\n",
        "max_len = max(len(max(train_sents, key=len)),len(max(test_sents, key=len)),len(max(val_sents, key=len)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Max train sentence length: 54\n",
            "Max val sentence length: 47\n",
            "Max test sentence length: 54\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cICIt2AGXOkw"
      },
      "source": [
        "bert_model = 'aubmindlab/bert-base-arabertv01'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yq807jlKiJ-5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4896db4d-062a-4cf9-b181-e2286316aede"
      },
      "source": [
        "from transformers import BertTokenizer,AutoTokenizer\n",
        "\n",
        "# Load the BERT tokenizer.\n",
        "print('Loading BERT tokenizer...')\n",
        "tokenizer = AutoTokenizer.from_pretrained(bert_model,do_lower_case=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading BERT tokenizer...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KHHNFmsv2lBg",
        "outputId": "ef1b8c30-c544-414f-e394-e3f7612aa863"
      },
      "source": [
        "print (tokenizer.tokenize('الاسلام'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['الاسلام']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yqwJ46-iABjK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "outputId": "11646fd5-59c0-4e9a-a5dc-4a415f07f129"
      },
      "source": [
        "#------------------------sents processing --------------------------\n",
        "#-------------------keep all tokens--------------\n",
        "\"\"\"\n",
        "def encode_sentence(sents,labels):\n",
        "  tokenized_sents = []\n",
        "  tokenized_labels = []\n",
        "\n",
        "  for i , sent in enumerate (sents):\n",
        "    tokenized_word = []\n",
        "    tokenized_label = []\n",
        "    tokenized_word.append(\"[CLS]\")\n",
        "    tokenized_label.append(0)\n",
        "    for j , word in enumerate (sent):\n",
        "      word_tokens = tokenizer.tokenize(word)\n",
        "      if len(word_tokens) > 0:\n",
        "        tokenized_word.extend(word_tokens)\n",
        "        # Use the real label id for the first token of the word, and padding ids for the remaining tokens\n",
        "        tokenized_label.extend([labels[i][j]] + [0] * (len(word_tokens) - 1))\n",
        "    tokenized_word.append(\"[SEP]\")\n",
        "    tokenized_label.append(0)\n",
        "    special_tokens_count = tokenizer.num_special_tokens_to_add()\n",
        "    if len(tokenized_word) > max_len - special_tokens_count:\n",
        "        tokenized_word = tokenized_word[: (max_len - special_tokens_count)]\n",
        "        tokenized_label = tokenized_label[: (max_len - special_tokens_count)]    \n",
        "    tokenized_sents.append(tokenized_word)\n",
        "    tokenized_labels.append(tokenized_label)\n",
        "\n",
        "  \n",
        "  return tokenized_sents, tokenized_labels\n",
        "\"\"\"\n",
        "\n",
        "def encode_sentence(sents,labels):\n",
        "  tokenized_sents = []\n",
        "  tokenized_labels = []\n",
        "  Fasttext_seq = []\n",
        "  for i , sent in enumerate (sents):\n",
        "    tokenized_word = []\n",
        "    tokenized_label = []\n",
        "    #word_seq = []\n",
        "    tokenized_word.append(\"[CLS]\")\n",
        "    tokenized_label.append(0)\n",
        "    #word_seq.append(0)\n",
        "    for j , word in enumerate (sent):\n",
        "      word_tokens = tokenizer.tokenize(word)\n",
        "      tokenized_word.extend (word_tokens)\n",
        "      tokenized_label.extend([labels[i][j]] + [0] * (len(word_tokens) - 1))\n",
        "      \"\"\"\n",
        "      if word in embedding.vocab:\n",
        "        word_seq.extend([embedding.vocab[word].index]+ [0] * (len(word_tokens) - 1))\n",
        "      else:\n",
        "        word_seq.extend([embedding.vocab['unk'].index]+ [0] * (len(word_tokens) - 1))\n",
        "      \"\"\"\n",
        "    tokenized_word.append(\"[SEP]\")\n",
        "    tokenized_label.append(0)\n",
        "    #word_seq.append(0)\n",
        "    tokenized_sents.append(tokenized_word)\n",
        "    tokenized_labels.append(tokenized_label)\n",
        "    #Fasttext_seq.append(word_seq)\n",
        "\n",
        "  \n",
        "  return tokenized_sents, tokenized_labels#, Fasttext_seq\n",
        "\n",
        "\"\"\"\n",
        "      word_tokens = tokenizer.tokenize(word)\n",
        "      max = 0\n",
        "      index = 0 \n",
        "      for k, t in enumerate (word_tokens):\n",
        "        if (len (t) > max): \n",
        "          max = len (t)\n",
        "          index = k\n",
        "      tokenized_word.extend (word_tokens)\n",
        "      for k, t in enumerate (word_tokens):\n",
        "        if (k == index): \n",
        "          tokenized_label.append(labels[i][j])\n",
        "        else:\n",
        "          tokenized_label.append(0)\n",
        "  \"\"\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\n      word_tokens = tokenizer.tokenize(word)\\n      max = 0\\n      index = 0 \\n      for k, t in enumerate (word_tokens):\\n        if (len (t) > max): \\n          max = len (t)\\n          index = k\\n      tokenized_word.extend (word_tokens)\\n      for k, t in enumerate (word_tokens):\\n        if (k == index): \\n          tokenized_label.append(labels[i][j])\\n        else:\\n          tokenized_label.append(0)\\n  '"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jfR67ymw4V09"
      },
      "source": [
        "\"\"\"\n",
        "tr_tokenized_sents , tr_tokenized_labels , tr_Fasttext_seq= encode_sentence(train_sents,train_labels)\n",
        "val_tokenized_sents , val_tokenized_labels, val_Fasttext_seq = encode_sentence(val_sents,val_labels)\n",
        "te_tokenized_sents , te_tokenized_labels, te_Fasttext_seq = encode_sentence(test_sents,test_labels)\n",
        "\"\"\"\n",
        "tr_tokenized_sents , tr_tokenized_labels = encode_sentence(train_sents,train_labels)\n",
        "val_tokenized_sents , val_tokenized_labels = encode_sentence(val_sents,val_labels)\n",
        "te_tokenized_sents , te_tokenized_labels = encode_sentence(test_sents,test_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XWzdyJXXclmQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d69961ae-cd75-4b75-f1f8-9d43b389b9e9"
      },
      "source": [
        "print ('tokenization result for sent#1:')\n",
        "print (train_sents[11])\n",
        "print (tr_tokenized_sents[11])\n",
        "#print (tokenized_labels[0])\n",
        "print (tr_tokenized_labels[11])\n",
        "\n",
        "\n",
        "#print (tr_Fasttext_seq[11])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tokenization result for sent#1:\n",
            "['الله', 'عليه', 'وسلم', 'في', 'صلاته', 'أو', 'نقص', 'وهذا', 'الكلام', 'مدرج', 'من', 'إبراهيم', 'وما', 'ذاك', 'ما', 'الذي', 'حدث', 'وهو', 'سؤال', 'من', 'لم', 'يشعر', 'بما', 'وقع', 'منه', 'ولا', 'يقين', 'عنده', 'به', 'ولا', 'غلبة', 'ظن', 'فثنى', 'رجليه', 'عطف', 'رجليه', 'وجلس', 'على']\n",
            "['[CLS]', 'الله', 'عليه', 'وسلم', 'في', 'صلات', '##ه', 'أو', 'نقص', 'وهذا', 'الكلام', 'مدرج', 'من', 'إبراهيم', 'وما', 'ذاك', 'ما', 'الذي', 'حدث', 'وهو', 'سؤال', 'من', 'لم', 'يشعر', 'بما', 'وقع', 'منه', 'ولا', 'يقين', 'عنده', 'به', 'ولا', 'غلب', '##ة', 'ظن', 'ف', '##ثنى', 'رجليه', 'عط', '##ف', 'رجليه', 'وجلس', 'على', '[SEP]']\n",
            "[0, 'B-Allah', 'O', 'O', 'O', 'O', 0, 'O', 'O', 'O', 'O', 'O', 'O', 'B-Pers', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 0, 'O', 'O', 0, 'O', 'O', 0, 'O', 'O', 'O', 0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J6QHWaDMnN4g",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cf71a9c2-88a9-4d15-8adb-fdac2cb0210b"
      },
      "source": [
        "print('Max tokinized sentence length:',len(max(tr_tokenized_sents, key=len)))\n",
        "print('Max tokinized sentence length:',len(max(te_tokenized_sents, key=len)))\n",
        "print('Max tokinized sentence length:',len(max(val_tokenized_sents, key=len)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Max tokinized sentence length: 86\n",
            "Max tokinized sentence length: 92\n",
            "Max tokinized sentence length: 75\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jeJTgvcRjkKZ"
      },
      "source": [
        "max_len = max(len(max(tr_tokenized_sents, key=len)),len(max(te_tokenized_sents, key=len)),len(max(val_tokenized_sents, key=len)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0RP50XlaeE1V"
      },
      "source": [
        "#---------------get id, mask, and seg -----------------------\n",
        "def sents_processing (sents):\n",
        "  inputs_ids = []\n",
        "  masks = []\n",
        "  seg_ids = []\n",
        "  for sent in sents:\n",
        "    l = max_len-len(sent)\n",
        "    inputs_ids.append (tokenizer.convert_tokens_to_ids([word for word in sent])+ [tokenizer.pad_token_id]*l)\n",
        "    masks.append([1 for word in sent]+ [0]*l)\n",
        "    seg_ids.append([0 for word in sent ]+ [0]*l)\n",
        "\n",
        "  return torch.tensor(inputs_ids, dtype=torch.long), torch.tensor(masks, dtype=torch.long), torch.tensor(seg_ids, dtype=torch.long)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zDdKWEo_7_BT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a6974431-7759-4121-b02d-553cc3804564"
      },
      "source": [
        "train_ids, train_masks, train_seg = sents_processing (tr_tokenized_sents)\n",
        "val_ids, val_masks, val_seg = sents_processing (val_tokenized_sents)\n",
        "test_ids, test_masks, test_seg = sents_processing (te_tokenized_sents)\n",
        "\n",
        "print (train_ids[0])\n",
        "print (train_masks[0])\n",
        "print (train_seg[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([17028,  2819,   883,  7356,   883,  3887, 11628,  8327,   466, 12199,\n",
            "          398,  3938,  4773, 21595,  4033,  4107, 21985, 26420, 24602,   192,\n",
            "         4358,   193,  4369,  1557,   731,  2245,   108, 30114,   660, 34226,\n",
            "         4055,  5071,  3075,   206,   810,  2075,   908,   883,  2811,  6067,\n",
            "          437, 14078, 31026, 11052,   631, 17641,   437,  2811,  6067,   437,\n",
            "         1269,  9461,  2520, 17030, 17029, 17029, 17029, 17029, 17029, 17029,\n",
            "        17029, 17029, 17029, 17029, 17029, 17029, 17029, 17029, 17029, 17029,\n",
            "        17029, 17029, 17029, 17029, 17029, 17029, 17029, 17029, 17029, 17029,\n",
            "        17029, 17029, 17029, 17029, 17029, 17029, 17029, 17029, 17029, 17029,\n",
            "        17029, 17029])\n",
            "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
            "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r-tXCFVCupS1"
      },
      "source": [
        "\n",
        "tag2idx = {t: i+1 for i, t in enumerate(tag_classes)}\n",
        "tag2idx[\"PAD\"] = 0\n",
        "idx2tag = {i: w for w, i in tag2idx.items()}\n",
        "def labels_processing (labels):\n",
        "\n",
        "  labels_ids = []\n",
        "  for label in labels:\n",
        "    l = max_len - len (label)\n",
        "    labels_ids.append ([tag2idx[tag] if tag != 0 else tag for tag in label] + [0]*l)\n",
        "  return torch.tensor(labels_ids, dtype=torch.long)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DGwvzRUe_op5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c376d88e-63bf-4f67-afc9-f4be9bb83bf2"
      },
      "source": [
        "train_labels = labels_processing (tr_tokenized_labels)\n",
        "test_labels = labels_processing (te_tokenized_labels)\n",
        "val_labels = labels_processing (val_tokenized_labels)\n",
        "\n",
        "print (train_labels[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([0, 2, 0, 2, 0, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 0, 2, 0, 2,\n",
            "        2, 2, 2, 0, 2, 8, 2, 2, 2, 2, 0, 2, 0, 0, 4, 4, 4, 4, 2, 4, 2, 4, 4, 4,\n",
            "        4, 4, 4, 4, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IGX1JhbsINIf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0608b188-c28f-467a-ab75-bd416b3ad311"
      },
      "source": [
        "print('Max inputs_ids length:',len(max(train_labels, key=len)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Max inputs_ids length: 92\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U2UP1mya662H"
      },
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "#define a batch size\n",
        "train_batch_size = 32\n",
        "test_batch_size = 8\n",
        "\n",
        "# wrap tensors\n",
        "#train_data = TensorDataset(train_ids, train_masks, train_seg, train_Fasttext_seq, train_labels)\n",
        "train_data = TensorDataset(train_ids, train_masks, train_seg, train_labels)\n",
        "\n",
        "# sampler for sampling the data during training\n",
        "train_sampler = RandomSampler(train_data)\n",
        "\n",
        "# dataLoader for train set\n",
        "train_dataloader = DataLoader(train_data, batch_size=train_batch_size, num_workers=2)\n",
        "\n",
        "# wrap tensors\n",
        "#val_data = TensorDataset(val_ids, val_masks, val_seg, val_Fasttext_seq, val_labels)\n",
        "val_data = TensorDataset(val_ids, val_masks, val_seg, val_labels)\n",
        "\n",
        "# sampler for sampling the data during training\n",
        "val_sampler = SequentialSampler(val_data)\n",
        "\n",
        "# dataLoader for validation set\n",
        "val_dataloader = DataLoader(val_data, batch_size=test_batch_size, num_workers=1)\n",
        "\n",
        "# wrap tensors\n",
        "#test_data = TensorDataset(test_ids, test_masks, test_seg, test_Fasttext_seq, test_labels)\n",
        "test_data = TensorDataset(test_ids, test_masks, test_seg, test_labels)\n",
        "\n",
        "# sampler for sampling the data during training\n",
        "test_sampler = SequentialSampler(test_data)\n",
        "\n",
        "# dataLoader for validation set\n",
        "test_dataloader = DataLoader(test_data, batch_size=test_batch_size, num_workers=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "izMzl4vh6HY-"
      },
      "source": [
        "from transformers import BertModel, BertConfig, AdamW\n",
        " \n",
        "bert = BertModel.from_pretrained(bert_model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jqnbxnmL6m_L"
      },
      "source": [
        "class BERTNERTagger(nn.Module):\n",
        "    def __init__(self,\n",
        "                 bert):\n",
        "        \n",
        "        super().__init__()\n",
        "        \n",
        "        self.bert = bert \n",
        "\n",
        "        self.gruUnits = 384\n",
        "      \n",
        "      # dropout layer\n",
        "        self.dropout = nn.Dropout(0.2)\n",
        "\n",
        "      # gru layer 1\n",
        "        self.bidirectional = True\n",
        "        self.gru_layers = 1\n",
        "        self.gruUnits = 200\n",
        "        self.gru_dropout = 0.1\n",
        "        self.gru = nn.GRU(768, self.gruUnits,  \n",
        "                          bidirectional = self.bidirectional, \n",
        "                          num_layers = self.gru_layers,\n",
        "                           dropout = 0.5)\n",
        "        \n",
        "        self.fc = nn.Linear(self.gruUnits * 2 if self.bidirectional else self.gruUnits, len(tag2idx))\n",
        "        #self.fc = nn.Linear(768, len(tag2idx))\n",
        "\n",
        "\n",
        "      #softmax activation function\n",
        "        self.softmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "\n",
        "        self.attn = nn.MultiheadAttention(\n",
        "                embed_dim=self.gruUnits * 2,\n",
        "                num_heads=4,\n",
        "                dropout=0.5\n",
        "            )\n",
        "        \n",
        "        self.crf = ConditionalRandomField(len(tag2idx),\n",
        "                            label_encoding=\"BIO\",\n",
        "                            idx2tag=idx2tag # Index to tag mapping\n",
        "                            )\n",
        "        #self.embedding = nn.Embedding.from_pretrained(weights, padding_idx = 0)\n",
        "        #self.embedding.weight.requires_grad = False\n",
        "        \n",
        "    def forward(self, input_id,masks, seg, targets=None):\n",
        "\n",
        "        pool, seq = self.bert(input_id, attention_mask=masks, token_type_ids=seg)\n",
        "\n",
        "        pool = self.dropout (pool)\n",
        "\n",
        "        #fast = self.embedding (Fasttext_seq)\n",
        "        \n",
        "        pool , h = self.gru(pool)\n",
        "        pool = self.dropout (pool)\n",
        "\n",
        "        #combined = self.dropout (combined)\n",
        "\n",
        "      # output layer\n",
        "        out = self.fc(pool)\n",
        "\n",
        "        best_tag_sequence = self.crf.best_viterbi_tag(out, masks)\n",
        "        \n",
        "        \n",
        "        class_probabilities = out * 0.0\n",
        "        for i, instance_tags in enumerate(best_tag_sequence):\n",
        "            for j, tag_id in enumerate(instance_tags[0][0]):\n",
        "                class_probabilities[i, j, int(tag_id)] = 1        \n",
        "\n",
        "        output = class_probabilities\n",
        "        \n",
        "        \n",
        "        if targets is not None:\n",
        "          loss= self.loss_fn(out, targets, masks)\n",
        "        return output, loss\n",
        "\n",
        "    def loss_fn(self, logits, target, masks):\n",
        "        log_likelihood = self.crf(logits, target, masks)\n",
        "        return -log_likelihood / logits.shape[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NuVMQzpwGnGh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "773d59ae-8319-4ff0-936a-d9e9236a786d"
      },
      "source": [
        "# pass the pre-trained BERT to our define architecture\n",
        "model = BERTNERTagger(bert)\n",
        "\n",
        "# push the model to GPU\n",
        "model = model.to(device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/rnn.py:61: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
            "  \"num_layers={}\".format(dropout, num_layers))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z0Zck_R_G75z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9f58be6b-f6a6-4135-a9e7-4d4e7574c4a7"
      },
      "source": [
        "from transformers import AdamW\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "EPOCHS = 10\n",
        "LEARNING_RATE = 5e-5\n",
        "WARMUP_RATIO = 0.1\n",
        "MAX_GRAD_NORM = 1.0\n",
        "param_optimizer = list(model.named_parameters())\n",
        "no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n",
        "optimizer_parameters = [\n",
        "    {\n",
        "        \"params\": [\n",
        "            p for n, p in param_optimizer if not any(nd in n for nd in no_decay)\n",
        "        ],\n",
        "        \"weight_decay\": 0.01,\n",
        "    },\n",
        "    {\n",
        "        \"params\": [\n",
        "            p for n, p in param_optimizer if any(nd in n for nd in no_decay)\n",
        "        ],\n",
        "        \"weight_decay\": 0.0,\n",
        "    },\n",
        "]\n",
        "\n",
        "num_train_steps = int(len(train_sents) / train_batch_size * EPOCHS)\n",
        "print('Number of training steps: ', num_train_steps)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
        "#optimizer = AdamW(optimizer_parameters, lr=LEARNING_RATE)\n",
        "scheduler = get_linear_schedule_with_warmup(\n",
        "    optimizer, num_warmup_steps=int(WARMUP_RATIO*num_train_steps), num_training_steps=num_train_steps\n",
        ")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of training steps:  1695\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P6DSJ1PROikI"
      },
      "source": [
        "def loss_fn(output, target, mask, num_labels):\n",
        "    lfn = nn.CrossEntropyLoss(ignore_index=0)\n",
        "    active_loss = mask.view(-1) == 1\n",
        "    active_logits = output.view(-1, num_labels)\n",
        "    active_labels = torch.where(\n",
        "        active_loss,\n",
        "        target.view(-1),\n",
        "        torch.tensor(lfn.ignore_index).type_as(target)\n",
        "    )\n",
        "    loss = lfn(active_logits, active_labels)\n",
        "    return loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "08CajfceFu9M"
      },
      "source": [
        "from sklearn.metrics import recall_score, precision_score, classification_report, accuracy_score, confusion_matrix, f1_score\n",
        "\n",
        "\n",
        "def align_predictions(predictions, label_ids):\n",
        "    preds = np.argmax(predictions, axis=2)\n",
        "\n",
        "    batch_size, seq_len = preds.shape\n",
        "\n",
        "    out_label_list = [[] for _ in range(batch_size)]\n",
        "    preds_list = [[] for _ in range(batch_size)]\n",
        "\n",
        "    for i in range(batch_size):\n",
        "        for j in range(seq_len):\n",
        "            if label_ids[i, j] != 0:\n",
        "                out_label_list[i].append(idx2tag[label_ids[i][j]])\n",
        "                preds_list[i].append(idx2tag[preds[i][j]])\n",
        "    return preds_list, out_label_list\n",
        "\n",
        "def y2label(zipped, mask=0):\n",
        "    out_true = []\n",
        "    out_pred = []\n",
        "    for zip_i in zipped:\n",
        "        a, b = tuple(zip_i)\n",
        "        if a != mask :\n",
        "            out_true.append(idx2tag[a].replace('B-','').replace('I-',''))\n",
        "            out_pred.append(idx2tag[b].replace('B-','').replace('I-',''))\n",
        "    return out_true, out_pred\n",
        "\n",
        "def compute_metrics(predictions,label_ids):\n",
        "    print (\"predictions:\" , predictions.shape)\n",
        "    print (\"label_ids:\" , label_ids.shape)\n",
        "    predictions = np.argmax(predictions, axis=2)\n",
        "    y_zipped = zip(np.array(label_ids).flat, np.array(predictions).flat)\n",
        "    preds_list, out_label_list = y2label(y_zipped)\n",
        "    print(classification_report(out_label_list, preds_list,digits=4))\n",
        "    return {\n",
        "        \"accuracy_score\": accuracy_score(out_label_list, preds_list),\n",
        "        \"precision\": precision_score(out_label_list, preds_list, average='macro'),\n",
        "        \"recall\": recall_score(out_label_list, preds_list, average='macro'),\n",
        "        \"f1\": f1_score(out_label_list, preds_list, average='macro'),\n",
        "    }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "liLdb1uxNb85"
      },
      "source": [
        "# function to train the model\n",
        "def train():\n",
        "  \n",
        "  model.train()\n",
        "\n",
        "  total_loss, total_accuracy = 0, 0\n",
        "  \n",
        "  # empty list to save model predictions\n",
        "  total_preds=[]\n",
        "  # iterate over batches\n",
        "  for step,batch in enumerate(train_dataloader):\n",
        "    \n",
        "    # progress update after every 50 batches.\n",
        "    if step % 50 == 0 and not step == 0:\n",
        "      print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(train_dataloader)))\n",
        "\n",
        "    # push the batch to gpu\n",
        "    batch = [r.to(device) for r in batch]\n",
        " \n",
        "    #sent_id, mask, seg, fast, labels = batch\n",
        "    sent_id, mask, seg, labels = batch\n",
        "\n",
        "    # clear previously calculated gradients \n",
        "    optimizer.zero_grad()        \n",
        "    # get model predictions for the current batch\n",
        "    preds, loss = model(sent_id, mask, seg, labels)\n",
        "    # compute the loss between actual and predicted values\n",
        "    # add on to the total loss\n",
        "    #loss = loss_fn(preds, labels, mask, len(tag2idx))\n",
        "    total_loss = total_loss + loss.item()\n",
        "\n",
        "    # backward pass to calculate the gradients\n",
        "    loss.backward()\n",
        "\n",
        "    # clip the the gradients to 1.0. It helps in preventing the exploding gradient problem\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "    # update parameters\n",
        "    optimizer.step()\n",
        "    scheduler.step()\n",
        "    # model predictions are stored on GPU. So, push it to CPU\n",
        "    preds=preds.detach().cpu().numpy()\n",
        "    # append the model predictions\n",
        "    total_preds.append(preds)\n",
        "\n",
        "  # compute the training loss of the epoch\n",
        "  avg_loss = total_loss / len(train_dataloader)\n",
        "  # predictions are in the form of (no. of batches, size of batch, no. of classes).\n",
        "  # reshape the predictions in form of (number of samples, no. of classes)\n",
        "  total_preds  = np.concatenate(total_preds, axis=0)\n",
        "\n",
        "  #returns the loss and predictions\n",
        "  return avg_loss, total_preds"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v9SlCYtgTWF9"
      },
      "source": [
        "# function for evaluating the model\n",
        "def evaluate(dataloader):\n",
        "  \n",
        "  print(\"\\nEvaluating...\")\n",
        "  \n",
        "  # deactivate dropout layers\n",
        "  model.eval()\n",
        "\n",
        "  total_loss, total_accuracy = 0, 0\n",
        "  \n",
        "  # empty list to save the model predictions\n",
        "  total_preds = []\n",
        "  total_labels = []\n",
        "  # iterate over batches\n",
        "  for step,batch in enumerate(dataloader):\n",
        "    \n",
        "    # Progress update every 50 batches.\n",
        "    if step % 50 == 0 and not step == 0:\n",
        "            \n",
        "      # Report progress.\n",
        "      print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(dataloader)))\n",
        "\n",
        "    # push the batch to gpu\n",
        "    batch = [t.to(device) for t in batch]\n",
        "\n",
        "    sent_id, mask, seg, labels = batch\n",
        "\n",
        "    # deactivate autograd\n",
        "    with torch.no_grad():\n",
        "      \n",
        "      # model predictions\n",
        "      preds , loss = model(sent_id, mask, seg, labels)\n",
        "      # compute the validation loss between actual and predicted values\n",
        "      #loss = loss_fn(preds, labels, mask, len(tag2idx))\n",
        "      total_loss = total_loss + loss  \n",
        "      preds = preds.detach().cpu().numpy()\n",
        "      labels=labels.detach().cpu().numpy()\n",
        "\n",
        "\n",
        "      total_preds.append(preds)\n",
        "      total_labels.append(labels)\n",
        "  # compute the validation loss of the epoch\n",
        "  avg_loss = total_loss / len(dataloader) \n",
        "  # reshape the predictions in form of (number of samples, no. of classes)\n",
        "  total_preds  = np.concatenate(total_preds, axis=0)\n",
        "  total_labels  = np.concatenate(total_labels, axis=0)\n",
        "\n",
        "  return avg_loss, total_preds, compute_metrics(total_preds,total_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LUyOvaQyQFXr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "76fbbae2-504d-431e-f20b-1a0a644cb268"
      },
      "source": [
        "epochs = EPOCHS\n",
        "MODEL_PATH = 'BERT_BGRU_softmax(ANERCorp_+_AQMAR).bin'\n",
        "# set initial loss to infinite\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "# empty lists to store training and validation loss of each epoch\n",
        "train_losses=[]\n",
        "valid_losses=[]\n",
        "best_f1 = 0\n",
        "#for each epoch\n",
        "for epoch in range(epochs):\n",
        "     \n",
        "    print('\\n Epoch {:} / {:}'.format(epoch + 1, epochs))\n",
        "    \n",
        "    #train model\n",
        "    train_loss, _ = train()\n",
        "   \n",
        "    #evaluate model\n",
        "    valid_loss, preds, eval_metrics = evaluate(val_dataloader)\n",
        "    \n",
        "    #save the best model\n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model.state_dict(), 'saved_weights.pt')\n",
        "    \n",
        "    # append training and validation loss\n",
        "    train_losses.append(train_loss)\n",
        "    valid_losses.append(valid_loss)\n",
        "    \n",
        "    print(f'\\nTraining Loss: {train_loss:.3f}')\n",
        "    print(f'Validation Loss: {valid_loss:.3f}')\n",
        "    print (eval_metrics)\n",
        "    if eval_metrics['f1'] > best_f1:\n",
        "      torch.save(model, MODEL_PATH)\n",
        "      best_f1 = eval_metrics['f1']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            " Epoch 1 / 10\n",
            "  Batch    50  of    170.\n",
            "  Batch   100  of    170.\n",
            "  Batch   150  of    170.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of     76.\n",
            "predictions: (603, 92, 9)\n",
            "label_ids: (603, 92)\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       Allah     0.9876    0.9903    0.9889       722\n",
            "        Clan     0.8667    0.8125    0.8387        80\n",
            "        Date     0.8444    0.7037    0.7677        54\n",
            "         Loc     0.7823    0.8779    0.8273       131\n",
            "       NatOb     0.5536    0.7045    0.6200        44\n",
            "           O     0.9962    0.9927    0.9944     18238\n",
            "        Pers     0.9759    0.9914    0.9836      3471\n",
            "     Prophet     0.9793    0.9683    0.9738       537\n",
            "\n",
            "    accuracy                         0.9893     23277\n",
            "   macro avg     0.8732    0.8802    0.8743     23277\n",
            "weighted avg     0.9897    0.9893    0.9894     23277\n",
            "\n",
            "\n",
            "Training Loss: 23.351\n",
            "Validation Loss: 2.083\n",
            "{'accuracy_score': 0.9893457060617777, 'precision': 0.8732431771072066, 'recall': 0.8801585959908681, 'f1': 0.874303935712077}\n",
            "\n",
            " Epoch 2 / 10\n",
            "  Batch    50  of    170.\n",
            "  Batch   100  of    170.\n",
            "  Batch   150  of    170.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of     76.\n",
            "predictions: (603, 92, 9)\n",
            "label_ids: (603, 92)\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       Allah     0.9903    0.9903    0.9903       724\n",
            "        Clan     0.9067    0.9315    0.9189        73\n",
            "        Date     0.7333    0.9706    0.8354        34\n",
            "         Loc     0.8367    0.9044    0.8693       136\n",
            "       NatOb     0.5714    0.8421    0.6809        38\n",
            "           O     0.9976    0.9936    0.9956     18246\n",
            "        Pers     0.9847    0.9926    0.9886      3498\n",
            "     Prophet     0.9831    0.9886    0.9858       528\n",
            "\n",
            "    accuracy                         0.9923     23277\n",
            "   macro avg     0.8755    0.9517    0.9081     23277\n",
            "weighted avg     0.9928    0.9923    0.9925     23277\n",
            "\n",
            "\n",
            "Training Loss: 1.363\n",
            "Validation Loss: 1.478\n",
            "{'accuracy_score': 0.992267044722258, 'precision': 0.8754830815730057, 'recall': 0.9517236989059461, 'f1': 0.908110347798526}\n",
            "\n",
            " Epoch 3 / 10\n",
            "  Batch    50  of    170.\n",
            "  Batch   100  of    170.\n",
            "  Batch   150  of    170.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of     76.\n",
            "predictions: (603, 92, 9)\n",
            "label_ids: (603, 92)\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       Allah     0.9876    0.9889    0.9883       723\n",
            "        Clan     0.9200    0.9324    0.9262        74\n",
            "        Date     0.9111    0.8039    0.8542        51\n",
            "         Loc     0.8503    0.9328    0.8897       134\n",
            "       NatOb     0.6786    0.8261    0.7451        46\n",
            "           O     0.9964    0.9954    0.9959     18191\n",
            "        Pers     0.9895    0.9881    0.9888      3531\n",
            "     Prophet     0.9793    0.9867    0.9830       527\n",
            "\n",
            "    accuracy                         0.9926     23277\n",
            "   macro avg     0.9141    0.9318    0.9214     23277\n",
            "weighted avg     0.9928    0.9926    0.9926     23277\n",
            "\n",
            "\n",
            "Training Loss: 0.841\n",
            "Validation Loss: 1.588\n",
            "{'accuracy_score': 0.9925677707608369, 'precision': 0.9140938583991556, 'recall': 0.9318020904534263, 'f1': 0.9213797153639379}\n",
            "\n",
            " Epoch 4 / 10\n",
            "  Batch    50  of    170.\n",
            "  Batch   100  of    170.\n",
            "  Batch   150  of    170.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of     76.\n",
            "predictions: (603, 92, 9)\n",
            "label_ids: (603, 92)\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       Allah     0.9890    0.9890    0.9890       724\n",
            "        Clan     0.9200    0.9200    0.9200        75\n",
            "        Date     0.9333    0.8077    0.8660        52\n",
            "         Loc     0.8571    0.9474    0.9000       133\n",
            "       NatOb     0.7500    0.7636    0.7568        55\n",
            "           O     0.9959    0.9958    0.9958     18175\n",
            "        Pers     0.9898    0.9887    0.9892      3530\n",
            "     Prophet     0.9868    0.9831    0.9850       533\n",
            "\n",
            "    accuracy                         0.9927     23277\n",
            "   macro avg     0.9277    0.9244    0.9252     23277\n",
            "weighted avg     0.9928    0.9927    0.9927     23277\n",
            "\n",
            "\n",
            "Training Loss: 0.610\n",
            "Validation Loss: 1.578\n",
            "{'accuracy_score': 0.9926966533487992, 'precision': 0.9277383651729164, 'recall': 0.9243992227091876, 'f1': 0.9252120059035355}\n",
            "\n",
            " Epoch 5 / 10\n",
            "  Batch    50  of    170.\n",
            "  Batch   100  of    170.\n",
            "  Batch   150  of    170.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of     76.\n",
            "predictions: (603, 92, 9)\n",
            "label_ids: (603, 92)\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       Allah     0.9903    0.9903    0.9903       724\n",
            "        Clan     0.9467    0.9221    0.9342        77\n",
            "        Date     0.9111    0.8367    0.8723        49\n",
            "         Loc     0.8571    0.9333    0.8936       135\n",
            "       NatOb     0.6964    0.8667    0.7723        45\n",
            "           O     0.9970    0.9952    0.9961     18207\n",
            "        Pers     0.9881    0.9929    0.9905      3509\n",
            "     Prophet     0.9906    0.9906    0.9906       531\n",
            "\n",
            "    accuracy                         0.9934     23277\n",
            "   macro avg     0.9222    0.9410    0.9300     23277\n",
            "weighted avg     0.9936    0.9934    0.9935     23277\n",
            "\n",
            "\n",
            "Training Loss: 0.423\n",
            "Validation Loss: 1.793\n",
            "{'accuracy_score': 0.9933840271512652, 'precision': 0.9221726933240125, 'recall': 0.9409712586293546, 'f1': 0.9299916804548362}\n",
            "\n",
            " Epoch 6 / 10\n",
            "  Batch    50  of    170.\n",
            "  Batch   100  of    170.\n",
            "  Batch   150  of    170.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of     76.\n",
            "predictions: (603, 92, 9)\n",
            "label_ids: (603, 92)\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       Allah     0.9890    0.9903    0.9896       723\n",
            "        Clan     0.8800    0.9296    0.9041        71\n",
            "        Date     0.9556    0.8269    0.8866        52\n",
            "         Loc     0.9048    0.8986    0.9017       148\n",
            "       NatOb     0.6964    0.8478    0.7647        46\n",
            "           O     0.9964    0.9960    0.9962     18180\n",
            "        Pers     0.9898    0.9929    0.9913      3515\n",
            "     Prophet     0.9944    0.9742    0.9842       542\n",
            "\n",
            "    accuracy                         0.9934     23277\n",
            "   macro avg     0.9258    0.9320    0.9273     23277\n",
            "weighted avg     0.9935    0.9934    0.9934     23277\n",
            "\n",
            "\n",
            "Training Loss: 0.294\n",
            "Validation Loss: 1.742\n",
            "{'accuracy_score': 0.9933840271512652, 'precision': 0.9257824982772636, 'recall': 0.9320487957987736, 'f1': 0.9273083094596959}\n",
            "\n",
            " Epoch 7 / 10\n",
            "  Batch    50  of    170.\n",
            "  Batch   100  of    170.\n",
            "  Batch   150  of    170.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of     76.\n",
            "predictions: (603, 92, 9)\n",
            "label_ids: (603, 92)\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       Allah     0.9903    0.9876    0.9890       726\n",
            "        Clan     0.9867    0.9367    0.9610        79\n",
            "        Date     0.9333    0.8077    0.8660        52\n",
            "         Loc     0.9048    0.9048    0.9048       147\n",
            "       NatOb     0.7857    0.8000    0.7928        55\n",
            "           O     0.9965    0.9965    0.9965     18173\n",
            "        Pers     0.9889    0.9923    0.9906      3514\n",
            "     Prophet     0.9868    0.9868    0.9868       531\n",
            "\n",
            "    accuracy                         0.9937     23277\n",
            "   macro avg     0.9466    0.9265    0.9359     23277\n",
            "weighted avg     0.9937    0.9937    0.9937     23277\n",
            "\n",
            "\n",
            "Training Loss: 0.207\n",
            "Validation Loss: 1.722\n",
            "{'accuracy_score': 0.9936847531898441, 'precision': 0.9466303259948265, 'recall': 0.9265473056578201, 'f1': 0.9359323968812899}\n",
            "\n",
            " Epoch 8 / 10\n",
            "  Batch    50  of    170.\n",
            "  Batch   100  of    170.\n",
            "  Batch   150  of    170.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of     76.\n",
            "predictions: (603, 92, 9)\n",
            "label_ids: (603, 92)\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       Allah     0.9903    0.9876    0.9890       726\n",
            "        Clan     0.9600    0.9351    0.9474        77\n",
            "        Date     0.9111    0.8367    0.8723        49\n",
            "         Loc     0.9048    0.9172    0.9110       145\n",
            "       NatOb     0.7321    0.8367    0.7810        49\n",
            "           O     0.9966    0.9960    0.9963     18183\n",
            "        Pers     0.9898    0.9915    0.9906      3520\n",
            "     Prophet     0.9849    0.9905    0.9877       528\n",
            "\n",
            "    accuracy                         0.9936     23277\n",
            "   macro avg     0.9337    0.9364    0.9344     23277\n",
            "weighted avg     0.9937    0.9936    0.9936     23277\n",
            "\n",
            "\n",
            "Training Loss: 0.126\n",
            "Validation Loss: 1.926\n",
            "{'accuracy_score': 0.9935988314645358, 'precision': 0.9337074908955649, 'recall': 0.936428355132044, 'f1': 0.9344071407882235}\n",
            "\n",
            " Epoch 9 / 10\n",
            "  Batch    50  of    170.\n",
            "  Batch   100  of    170.\n",
            "  Batch   150  of    170.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of     76.\n",
            "predictions: (603, 92, 9)\n",
            "label_ids: (603, 92)\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       Allah     0.9903    0.9903    0.9903       724\n",
            "        Clan     0.9600    0.9231    0.9412        78\n",
            "        Date     0.9111    0.8367    0.8723        49\n",
            "         Loc     0.9048    0.9172    0.9110       145\n",
            "       NatOb     0.6964    0.8667    0.7723        45\n",
            "           O     0.9966    0.9961    0.9964     18183\n",
            "        Pers     0.9904    0.9906    0.9905      3525\n",
            "     Prophet     0.9812    0.9867    0.9839       528\n",
            "\n",
            "    accuracy                         0.9936     23277\n",
            "   macro avg     0.9289    0.9384    0.9322     23277\n",
            "weighted avg     0.9937    0.9936    0.9936     23277\n",
            "\n",
            "\n",
            "Training Loss: 0.084\n",
            "Validation Loss: 1.938\n",
            "{'accuracy_score': 0.9935558706018817, 'precision': 0.928850175596238, 'recall': 0.9384408913209326, 'f1': 0.9322373348574762}\n",
            "\n",
            " Epoch 10 / 10\n",
            "  Batch    50  of    170.\n",
            "  Batch   100  of    170.\n",
            "  Batch   150  of    170.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of     76.\n",
            "predictions: (603, 92, 9)\n",
            "label_ids: (603, 92)\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       Allah     0.9890    0.9903    0.9896       723\n",
            "        Clan     0.9733    0.9241    0.9481        79\n",
            "        Date     0.9111    0.8367    0.8723        49\n",
            "         Loc     0.9184    0.9184    0.9184       147\n",
            "       NatOb     0.6964    0.8478    0.7647        46\n",
            "           O     0.9969    0.9963    0.9966     18184\n",
            "        Pers     0.9904    0.9918    0.9911      3521\n",
            "     Prophet     0.9831    0.9886    0.9858       528\n",
            "\n",
            "    accuracy                         0.9939     23277\n",
            "   macro avg     0.9323    0.9367    0.9333     23277\n",
            "weighted avg     0.9940    0.9939    0.9939     23277\n",
            "\n",
            "\n",
            "Training Loss: 0.036\n",
            "Validation Loss: 2.017\n",
            "{'accuracy_score': 0.9938565966404606, 'precision': 0.9323077888417054, 'recall': 0.9367446744386132, 'f1': 0.9333196149949079}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NZl0SZmFTRQA",
        "outputId": "97436f2a-b29c-4818-cb41-c50c46d235b4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#model.load_state_dict(torch.load(MODEL_PATH))\n",
        "test_loss, preds, test_metrics = evaluate (test_dataloader)\n",
        "print(f\"Test Loss = {test_loss}\")\n",
        "print(test_metrics)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Evaluating...\n",
            "  Batch    50  of     84.\n",
            "predictions: (670, 92, 9)\n",
            "label_ids: (670, 92)\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       Allah     0.9910    0.9784    0.9847       788\n",
            "        Clan     0.9029    0.9894    0.9442        94\n",
            "        Date     0.8667    0.9155    0.8904        71\n",
            "         Loc     0.9310    0.9153    0.9231       118\n",
            "       NatOb     0.8571    0.9153    0.8852        59\n",
            "           O     0.9965    0.9962    0.9964     20254\n",
            "        Pers     0.9883    0.9901    0.9892      3751\n",
            "     Prophet     0.9874    0.9791    0.9832       718\n",
            "\n",
            "    accuracy                         0.9935     25853\n",
            "   macro avg     0.9401    0.9599    0.9495     25853\n",
            "weighted avg     0.9936    0.9935    0.9936     25853\n",
            "\n",
            "Test Loss = 2.1762871742248535\n",
            "{'accuracy_score': 0.9935404015007929, 'precision': 0.9401191577633068, 'recall': 0.9599102230017789, 'f1': 0.9495494396368983}\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}